# /api/routers/import_api.py
"""
API Router ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI-Powered Smart ETL System
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Import ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel/CSV ‡πÅ‡∏•‡∏∞ AI Transformation
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, File, UploadFile, HTTPException, Body, Depends, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from core.services.import_service import import_service
from core.database.mongodb_manager import MongoDBManager
from core.database.qdrant_manager import QdrantManager
from ..dependencies import get_mongo_manager, get_qdrant_manager

router = APIRouter(tags=["Admin :: Bulk Import"])


# =============================================================================
# Pydantic Models
# =============================================================================

class RawPreviewResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö raw file preview"""
    columns: List[str] = Field(..., description="‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ columns ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
    preview_rows: List[Dict[str, Any]] = Field(..., description="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (max 10 rows)")
    total_rows: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    showing_rows: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á")
    filename: str = Field(..., description="‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö")


class AITransformRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI transformation"""
    raw_data: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å preview")
    target_fields: List[str] = Field(
        ..., 
        description="Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏õ",
        min_length=1,
        max_length=15
    )


class AITransformResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI transformation result"""
    original_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö)")
    transformed_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    target_fields: List[str] = Field(..., description="Fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
    total_processed: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")


class ConfirmSaveRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö confirm ‡πÅ‡∏•‡∏∞ save ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    transformed_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡∏û‡∏£‡πâ‡∏≠‡∏° save)")


class ConfirmSaveResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö save result"""
    success: bool
    saved_count: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà save ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    failed_count: int = Field(0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà save ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    message: str


class PDFExtractResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF text extraction ‡πÅ‡∏•‡∏∞ AI fill"""
    success: bool
    extracted_text: Optional[str] = Field(None, description="Text ‡∏ó‡∏µ‡πà extract ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å PDF")
    page_count: int = Field(0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô PDF")
    ai_data: Optional[Dict[str, Any]] = Field(None, description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI extract ‡πÑ‡∏î‡πâ")
    message: str


class AIFillFormRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    partial_data: Dict[str, Any] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡∏°‡∏≤")
    target_fields: List[str] = Field(
        default=["title", "category", "topic", "summary", "keywords"],
        description="Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°"
    )
    use_web_search: bool = Field(
        default=False,
        description="‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Google Search ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
    )


class AIFillFormResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI fill form"""
    success: bool
    filled_data: Dict[str, Any] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ")
    message: str


class DocumentScanResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document scan"""
    success: bool
    page_count: int = Field(0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    text_preview: str = Field("", description="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    entries: List[Dict[str, str]] = Field(default=[], description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ entries ‡∏ó‡∏µ‡πà AI ‡∏û‡∏ö")
    message: str
    ai_suggested_count: int = Field(0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà AI ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")


class DocumentExtractRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document extract"""
    document_text: str = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    entries: List[Dict[str, str]] = Field(..., description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ entries ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ extract")
    target_fields: List[str] = Field(..., description="Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI extract")


class DocumentExtractResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document extract"""
    success: bool
    data: List[Dict[str, Any]] = Field(default=[], description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà extract ‡πÑ‡∏î‡πâ")
    message: str

# =============================================================================
# Target Fields Configuration - ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö MongoDB Schema ‡∏à‡∏£‡∏¥‡∏á
# =============================================================================

# Core Fields - ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô Database
CORE_FIELDS = [
    {
        "key": "title", 
        "label": "Title (‡∏ä‡∏∑‡πà‡∏≠)", 
        "description": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà/‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "category", 
        "label": "Category (‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)", 
        "description": "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å, ‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£, ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß, ‡∏ß‡∏±‡∏î",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "topic", 
        "label": "Topic (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏¢‡πà‡∏≠‡∏¢)", 
        "description": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà, ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠, ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "summary", 
        "label": "Summary (‡∏™‡∏£‡∏∏‡∏õ)", 
        "description": "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß",
        "type": "textarea",
        "required": False,
        "group": "core"
    },
    {
        "key": "keywords", 
        "label": "Keywords (‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤)", 
        "description": "‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ comma ‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏±‡∏î,‡∏ô‡πà‡∏≤‡∏ô,‡∏à‡∏¥‡∏ï‡∏£‡∏Å‡∏£‡∏£‡∏°",
        "type": "tags",
        "required": False,
        "group": "core"
    },
]

# Detail Fields - ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô details[] item)
DETAIL_FIELDS = [
    {
        "key": "detail_overview",
        "label": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
        "description": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤",
        "type": "detail",
        "heading": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
        "group": "details"
    },
    {
        "key": "detail_location",
        "label": "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "description": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà ‡∏û‡∏¥‡∏Å‡∏±‡∏î GPS ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "type": "detail",
        "heading": "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "group": "details"
    },
    {
        "key": "detail_hours_contact",
        "label": "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠",
        "description": "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£ Line Facebook",
        "type": "detail",
        "heading": "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠",
        "group": "details"
    },
    {
        "key": "detail_highlights",
        "label": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î",
        "description": "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥/‡∏î‡∏π",
        "type": "detail",
        "heading": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î",
        "group": "details"
    },
    {
        "key": "detail_price",
        "label": "‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
        "description": "‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏° ‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏´‡∏≤‡∏£",
        "type": "detail",
        "heading": "‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
        "group": "details"
    },
    {
        "key": "detail_atmosphere",
        "label": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå",
        "description": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà",
        "type": "detail",
        "heading": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå",
        "group": "details"
    },
    {
        "key": "detail_facilities",
        "label": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "description": "‡∏ó‡∏µ‡πà‡∏à‡∏≠‡∏î‡∏£‡∏ñ WiFi ‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥ ‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "type": "detail",
        "heading": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "group": "details"
    },
    {
        "key": "detail_tips",
        "label": "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "description": "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏°‡∏≤‡πÄ‡∏¢‡∏∑‡∏≠‡∏ô ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
        "type": "detail",
        "heading": "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "group": "details"
    },
]

# ‡∏£‡∏ß‡∏° Fields ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
ALL_CONFIGURABLE_FIELDS = CORE_FIELDS + DETAIL_FIELDS


# =============================================================================
# API Endpoints
# =============================================================================

@router.get("/target-fields", tags=["Admin :: Bulk Import"])
async def get_target_fields():
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Target Fields ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°
    - core: ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏´‡∏•‡∏±‡∏Å (title, category, topic, summary, keywords)
    - details: ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô details[] item)
    """
    return {
        "core_fields": CORE_FIELDS,
        "detail_fields": DETAIL_FIELDS,
        "all_fields": ALL_CONFIGURABLE_FIELDS,
        "total": len(ALL_CONFIGURABLE_FIELDS)
    }


@router.post("/preview-raw", response_model=RawPreviewResponse, tags=["Admin :: Bulk Import"])
async def preview_raw_file(file: UploadFile = File(...)):
    """
    üì§ Upload ‡πÅ‡∏•‡∏∞ Preview ‡πÑ‡∏ü‡∏•‡πå Excel/CSV
    
    ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á preview ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏Å‡πà‡∏≠‡∏ô AI transform
    """
    # Validate file type using File Validator (Magic Number Check)
    from utils.file_validator import verify_file_signature
    import io
    
    if not await verify_file_signature(file):
        raise HTTPException(
            status_code=400, 
            detail=f"‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (Invalid signature for {file.filename})"
        )
    
    try:
        # Read file with size limit (Anti-DoS)
        MAX_FILE_SIZE = 200 * 1024 * 1024 # 200 MB (Requested by Admin)
        content = io.BytesIO()
        size = 0
        CHUNK_SIZE = 1024 * 1024 # 1MB
        
        while chunk := await file.read(CHUNK_SIZE):
            size += len(chunk)
            if size > MAX_FILE_SIZE:
                raise HTTPException(status_code=413, detail=f"‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Max {MAX_FILE_SIZE/1024/1024} MB)")
            content.write(chunk)
            
        content.seek(0)
        file_content = content.read() # Load verified content for parsing
        
        if not file_content:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
        
        # Parse file in thread pool (blocking operation)
        raw_data = await asyncio.to_thread(
            import_service.parse_file,
            file_content,
            file.filename
        )
        
        if not raw_data:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
        
        # Generate preview
        preview = import_service.get_preview(raw_data, max_rows=10)
        
        return RawPreviewResponse(
            columns=preview["columns"],
            preview_rows=preview["preview_rows"],
            total_rows=preview["total_rows"],
            showing_rows=preview["showing_rows"],
            filename=file.filename
        )
        
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {str(e)}")


@router.post("/ai-transform", response_model=AITransformResponse, tags=["Admin :: Bulk Import"])
async def ai_transform_data(request: AITransformRequest):
    """
    ü§ñ AI Transform: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏•‡∏á Target Fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    
    Update: 
    - ‚ö° Smart Bypass: ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏°‡∏µ Fields ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏° Target ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏° AI ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ (Direct Import)
    - ü§ñ AI Fallback: ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö ‡∏´‡∏£‡∏∑‡∏≠ Field ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‡∏à‡∏∞‡πÉ‡∏ä‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢ Map ‡πÉ‡∏´‡πâ
    """
    if not request.raw_data:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    
    if not request.target_fields:
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Target Fields ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 field")
    
    # Validate target fields
    valid_field_keys = [f["key"] for f in ALL_CONFIGURABLE_FIELDS]
    for field in request.target_fields:
        if field.startswith("custom_"): continue
        if field not in valid_field_keys:
            raise HTTPException(status_code=400, detail=f"Invalid field: {field}")
    
    try:
        # 1. ‚ö° Smart Check: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ AI ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÜ ‡∏ß‡πà‡∏≤‡∏°‡∏µ Key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Target Fields ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        sample_size = min(5, len(request.raw_data))
        sample_rows = request.raw_data[:sample_size]
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Field ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (Case-insensitive check)
        direct_map_count = 0
        target_set = set(f.lower() for f in request.target_fields)
        
        for row in sample_rows:
            row_keys = set(k.lower() for k in row.keys())
            # ‡∏ñ‡πâ‡∏≤ row ‡∏ô‡∏µ‡πâ‡∏°‡∏µ keys ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° target_fields ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 80% (‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏£‡∏ö)
            common = row_keys.intersection(target_set)
            if len(common) >= len(target_set) * 0.8: # Threshold 80%
                direct_map_count += 1
        
        # ‡∏ñ‡πâ‡∏≤ Data ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏Å‡∏¥‡∏ô 80% ‡∏Ç‡∏≠‡∏á Sample) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß -> Bypass AI
        is_structured_data = direct_map_count >= (sample_size * 0.8)
        
        transformed = []
        
        if is_structured_data:
            logging.info(f"‚ö° [ImportAPI] Smart Bypass Active! ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô -> ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô AI")
            
            # Direct Map Logic
            for row in request.raw_data:
                # Normalize keys to lowercase for matching
                row_lower = {k.lower(): v for k, v in row.items()}
                
                new_row = {}
                for field in request.target_fields:
                    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ value ‡∏à‡∏≤‡∏Å key ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (case-insensitive)
                    # ‡πÄ‡∏ä‡πà‡∏ô target="title", data ‡∏°‡∏µ "Title" -> match
                    field_lower = field.lower()
                    if field_lower in row_lower:
                        new_row[field] = row_lower[field_lower]
                    else:
                        new_row[field] = None # ‡∏´‡∏£‡∏∑‡∏≠ "" ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                
                # Keep original data for reference (optional) or merge missing columns
                # for k, v in row.items():
                #     if k not in new_row: new_row[k] = v
                    
                transformed.append(new_row)
                
        else:
            # 2. ü§ñ AI Process Logic (‡πÄ‡∏î‡∏¥‡∏°)
            from core.services.ai_mapper_service import ai_mapper_service
            
            logging.info(f"ü§ñ [ImportAPI] Data is unstructured -> Using Gemini AI for {len(request.raw_data)} rows")
            
            transformed = await ai_mapper_service.transform_batch(
                rows=request.raw_data,
                target_fields=request.target_fields,
                concurrency=8 
            )
        
        logging.info(f"‚úÖ [ImportAPI] Transform ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(transformed)} ‡πÅ‡∏ñ‡∏ß (Mode: {'Direct' if is_structured_data else 'AI'})")
        
        return AITransformResponse(
            original_rows=request.raw_data,
            transformed_rows=transformed,
            target_fields=request.target_fields,
            total_processed=len(transformed)
        )
        
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] Transform Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Transform Failed: {str(e)}")


@router.post("/confirm-save", response_model=ConfirmSaveResponse, tags=["Admin :: Bulk Import"])
async def confirm_save_data(
    request: ConfirmSaveRequest,
    db: MongoDBManager = Depends(get_mongo_manager),
    vector_db: QdrantManager = Depends(get_qdrant_manager)
):
    """
    üíæ Confirm ‡πÅ‡∏•‡∏∞ Save ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á Database
    
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Vector ‡πÉ‡∏ô Qdrant
    """
    if not request.transformed_rows:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
    
    saved_count = 0
    failed_count = 0
    errors = []
    
    for idx, row in enumerate(request.transformed_rows):
        try:
            # Remove internal fields
            clean_row = {k: v for k, v in row.items() if not k.startswith("_")}
            
            # Generate slug from title (new schema) or name (old)
            title = clean_row.get("title") or clean_row.get("name", "")
            if not title:
                title = f"imported-item-{idx + 1}"
            
            # Create slug (kebab-case)
            import re
            slug = re.sub(r'[^a-zA-Z0-9\u0E00-\u0E7F\s-]', '', title.lower())
            slug = re.sub(r'[\s]+', '-', slug.strip())
            slug = slug[:50]  # Limit length
            if not slug:
                slug = f"item-{idx + 1}"
            
            # Check for duplicate slug and secure uniqueness
            original_slug = slug
            import uuid
            
            # Check if slug exists in DB (loop to ensure uniqueness)
            retry_count = 0
            while retry_count < 5:
                existing = await asyncio.to_thread(db.get_location_by_slug, slug)
                if not existing:
                    break
                
                # If exists, append random suffix
                suffix = uuid.uuid4().hex[:6]
                slug = f"{original_slug}-{suffix}"
                retry_count += 1
            
            if retry_count >= 5:
                # Fallback if still busy
                slug = f"{original_slug}-{uuid.uuid4().hex[:12]}"
            
            # Build location document matching new schema
            location_doc = {
                "slug": slug,
                "title": title,
                "category": clean_row.get("category", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"),
                "topic": clean_row.get("topic") or clean_row.get("sub_topic", "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"),
                "summary": clean_row.get("summary") or _build_summary(clean_row),
                "keywords": _extract_keywords(clean_row),
                "details": _build_details(clean_row),
                "metadata": {
                    "image_prefix": slug,
                    "imported_via": "bulk_import",
                    "source_fields": list(clean_row.keys())
                }
            }
            
            # Save to MongoDB
            mongo_id = await asyncio.to_thread(
                db.add_location,
                location_doc
            )
            
            if not mongo_id:
                raise Exception("MongoDB insert returned None")
            
            # Create vector in Qdrant
            try:
                desc_for_vector = f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {location_doc['title']}\n‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {location_doc['topic']}\n‡∏™‡∏£‡∏∏‡∏õ: {location_doc['summary']}"
                await vector_db.upsert_location(mongo_id=mongo_id, description=desc_for_vector)
            except Exception as ve:
                logging.warning(f"‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Vector ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {mongo_id}: {ve}")
            
            saved_count += 1
            logging.info(f"‚úÖ Saved: {location_doc['title']} (slug: {slug})")
            
        except Exception as e:
            failed_count += 1
            error_msg = f"Row {idx + 1}: {str(e)}"
            errors.append(error_msg)
            logging.error(f"‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {idx + 1} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
    
    # Build result message
    if failed_count == 0:
        message = f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {saved_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£!"
    else:
        message = f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {saved_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß {failed_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
        if errors:
            message += f". Errors: {'; '.join(errors[:3])}"
    
    return ConfirmSaveResponse(
        success=failed_count == 0,
        saved_count=saved_count,
        failed_count=failed_count,
        message=message
    )


@router.post("/pdf-extract", response_model=PDFExtractResponse, tags=["Admin :: Bulk Import"])
async def extract_from_pdf(file: UploadFile = File(...)):
    """
    üìÑ Extract ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Manual Entry

    1. ‡∏≠‡πà‡∏≤‡∏ô PDF ‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤
    2. ‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞ extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á fields ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    3. ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏Å‡∏•‡∏á‡∏ü‡∏≠‡∏£‡πå‡∏°
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå")
    
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå PDF ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
    
    try:
        # Read PDF content
        file_content = await file.read()
        
        if not file_content:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
        
        # Extract text from PDF
        from core.services.pdf_reader_service import pdf_reader_service
        extracted_text = pdf_reader_service.extract_text(file_content)
        page_count = pdf_reader_service.get_page_count(file_content)
        
        if not extracted_text.strip():
            return PDFExtractResponse(
                success=False,
                extracted_text="",
                page_count=page_count,
                ai_data=None,
                message="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô PDF (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠ scanned document)"
            )
        
        # Use AI to extract data from PDF text
        from core.services.ai_mapper_service import ai_mapper_service
        
        target_fields = ["title", "category", "topic", "summary", "keywords", 
                        "detail_overview", "detail_location", "detail_hours_contact",
                        "detail_highlights", "detail_price"]
        
        ai_data = await ai_mapper_service.extract_from_document(
            document_text=extracted_text,
            target_fields=target_fields
        )
        
        logging.info(f"‚úÖ [ImportAPI] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF: {page_count} ‡∏´‡∏ô‡πâ‡∏≤, AI ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len([v for v in ai_data.values() if v])} ‡∏ü‡∏¥‡∏•‡∏î‡πå")
        logging.info(f"üìÑ [ImportAPI] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å AI: {ai_data}")
        
        return PDFExtractResponse(
            success=True,
            extracted_text=extracted_text[:2000],  # Limit text preview
            page_count=page_count,
            ai_data=ai_data,
            message=f"‡∏≠‡πà‡∏≤‡∏ô PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {page_count} ‡∏´‡∏ô‡πâ‡∏≤"
        )
        
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô PDF: {str(e)}")


@router.post("/ai-fill-form", response_model=AIFillFormResponse, tags=["Admin :: Bulk Import"])
async def ai_fill_form(request: AIFillFormRequest):
    """
    ü§ñ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°
    
    ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô (‡πÄ‡∏ä‡πà‡∏ô title, details) ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏° fields ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
    ‡∏ñ‡πâ‡∏≤ use_web_search=True ‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google ‡∏î‡πâ‡∏ß‡∏¢
    """
    if not request.partial_data:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    
    try:
        from core.services.ai_mapper_service import ai_mapper_service
        
        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏õ‡πá‡∏ô text
        input_text = "\n".join([
            f"{k}: {v}" for k, v in request.partial_data.items() 
            if v and str(v).strip()
        ])
        
        if not input_text.strip():
            return AIFillFormResponse(
                success=False,
                filled_data={},
                message="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
            )
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ extract
        if request.use_web_search:
            # üåê ‡πÉ‡∏ä‡πâ Google Custom Search ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            from core.services.web_search_service import web_search_service
            
            search_query = request.partial_data.get("title", "") or request.partial_data.get("details", "")
            search_query = f"{search_query} ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß"  # ‡πÄ‡∏û‡∏¥‡πà‡∏° context
            
            logging.info(f"üåê [ImportAPI] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: {search_query}")
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å Google
            web_results = await web_search_service.search_and_summarize(search_query)
            
            if web_results:
                # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
                combined_text = f"{input_text}\n\n{web_results}"
                target_fields = request.target_fields + ["detail_overview", "detail_location", 
                                "detail_hours_contact", "detail_highlights", "detail_price"]
            else:
                combined_text = input_text
                target_fields = request.target_fields
                logging.warning("‚ö†Ô∏è [ImportAPI] ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
            
            ai_data = await ai_mapper_service.extract_from_document(
                document_text=combined_text,
                target_fields=target_fields
            )
        else:
            # üìù ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            ai_data = await ai_mapper_service.extract_from_document(
                document_text=input_text,
                target_fields=request.target_fields
            )
        
        # ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
        filled_data = {**request.partial_data}
        for field, value in ai_data.items():
            if value and (field not in filled_data or not filled_data[field]):
                filled_data[field] = value
        
        filled_count = len([v for v in ai_data.values() if v])
        method = "üåê Web Search" if request.use_web_search else "üìù Local"
        logging.info(f"‚úÖ [ImportAPI] {method} - AI ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {filled_count} ‡∏ü‡∏¥‡∏•‡∏î‡πå")
        
        return AIFillFormResponse(
            success=True,
            filled_data=filled_data,
            message=f"AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {filled_count} fields" + (" (‡∏Ñ‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö)" if request.use_web_search else "")
        )
        
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ AI ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"AI ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}")


# =============================================================================
# Document Import Endpoints - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF/DOC ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢ entries
# =============================================================================

@router.post("/document-scan", response_model=DocumentScanResponse)
async def document_scan(
    file: UploadFile = File(..., description="‡πÑ‡∏ü‡∏•‡πå PDF ‡∏´‡∏£‡∏∑‡∏≠ DOC/DOCX"),
    target_count: int = Form(default=0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (0 = AI ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)")
):
    """
    üìÑ Scan ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF/DOC ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ AI ‡∏£‡∏∞‡∏ö‡∏∏ entries ‡∏ó‡∏µ‡πà‡∏û‡∏ö
    
    1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    2. AI ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠/‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
    3. ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö entries ‡∏ó‡∏µ‡πà‡∏û‡∏ö (‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ)
    """
    # Validate file type
    from core.services.doc_reader_service import doc_reader_service
    
    if not file.filename:
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå")
    
    if not doc_reader_service.is_supported(file.filename):
        raise HTTPException(
            status_code=400, 
            detail=f"‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå PDF, DOC, DOCX ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
        )
    
    try:
        # Read file
        file_bytes = await file.read()
        
        if len(file_bytes) == 0:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
        
        # Extract text
        extracted_text, page_count = doc_reader_service.extract_text(file_bytes, file.filename)
        
        if not extracted_text.strip():
            return DocumentScanResponse(
                success=False,
                page_count=page_count,
                text_preview="",
                entries=[],
                message="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠ scanned document)"
            )
        
        # AI detect entries with target count
        from core.services.ai_mapper_service import ai_mapper_service
        
        entries = await ai_mapper_service.detect_entries(extracted_text, target_count=target_count if target_count > 0 else None)
        
        ai_suggested_count = len(entries)
        logging.info(f"‚úÖ [ImportAPI] ‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {page_count} ‡∏´‡∏ô‡πâ‡∏≤, ‡∏û‡∏ö {len(entries)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {target_count})")
        
        return DocumentScanResponse(
            success=True,
            page_count=page_count,
            text_preview=extracted_text[:2000],
            entries=entries,
            message=f"‡∏û‡∏ö {len(entries)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {page_count} ‡∏´‡∏ô‡πâ‡∏≤",
            ai_suggested_count=ai_suggested_count
        )
        
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {str(e)}")


@router.post("/document-extract", response_model=DocumentExtractResponse)
async def document_extract(request: DocumentExtractRequest):
    """
    üìä Extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° entries ‡πÅ‡∏•‡∏∞ fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    
    1. ‡∏£‡∏±‡∏ö entries ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏à‡∏≤‡∏Å document-scan ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏≠‡∏á)
    2. AI extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ entry ‡∏ï‡∏≤‡∏° fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    3. ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö table data ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    if not request.entries:
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ entries ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ extract")
    
    if not request.target_fields:
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ extract")
    
    try:
        from core.services.ai_mapper_service import ai_mapper_service
        
        results = await ai_mapper_service.extract_multiple_entries(
            document_text=request.document_text,
            entries=request.entries,
            target_fields=request.target_fields
        )
        
        logging.info(f"‚úÖ [ImportAPI] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        return DocumentExtractResponse(
            success=True,
            data=results,
            message=f"Extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
        )
        
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ extract: {str(e)}")


@router.post("/document-stream")
async def document_stream(
    file: UploadFile = File(..., description="‡πÑ‡∏ü‡∏•‡πå PDF ‡∏´‡∏£‡∏∑‡∏≠ DOC/DOCX"),
    target_count: int = Form(default=5, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"),
    target_fields: str = Form(default="title,category,topic,summary,keywords", description="Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (comma-separated)")
):
    """
    üìä SSE Streaming: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‚Üí ‡∏™‡πÅ‡∏Å‡∏ô ‚Üí Extract ‡∏ó‡∏µ‡∏•‡∏∞ entry ‚Üí Stream ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    
    ‡∏™‡πà‡∏á SSE events:
    - {"type": "scan", "data": {"page_count": 5, "total_entries": 5}}
    - {"type": "entry", "data": {"index": 0, "title": "...", ...}}
    - {"type": "done", "data": {"total": 5}}
    - {"type": "error", "data": {"message": "..."}}
    """
    import json
    from core.services.doc_reader_service import doc_reader_service
    from core.services.ai_mapper_service import ai_mapper_service
    
    # Read file BEFORE streaming (to avoid closed file error)
    file_bytes = await file.read()
    filename = file.filename
    
    async def generate():
        try:
            # 1. Validate file
            if len(file_bytes) == 0:
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': '‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤'}})}\n\n"
                return
            
            # 2. Extract text
            extracted_text, page_count = doc_reader_service.extract_text(file_bytes, filename)
            if not extracted_text.strip():
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£'}})}\n\n"
                return
            
            # 3. Detect entries
            entries = await ai_mapper_service.detect_entries(extracted_text, target_count=target_count)
            
            # Send scan result
            yield f"data: {json.dumps({'type': 'scan', 'data': {'page_count': page_count, 'total_entries': len(entries)}})}\n\n"
            
            # 4. Extract each entry and stream
            fields_list = [f.strip() for f in target_fields.split(',') if f.strip()]
            
            for idx, entry in enumerate(entries):
                try:
                    # Extract this entry
                    entry_title = entry.get("title", "")
                    search_prompt = f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {entry_title}\n‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {entry.get('description', '')}"
                    combined = f"{search_prompt}\n\n{extracted_text}"
                    
                    extracted = await ai_mapper_service.extract_from_document(
                        document_text=combined,
                        target_fields=fields_list
                    )
                    
                    # Ensure title is set
                    extracted['title'] = extracted.get('title') or entry_title
                    extracted['_index'] = idx
                    
                    # Send entry result
                    yield f"data: {json.dumps({'type': 'entry', 'data': extracted}, ensure_ascii=False)}\n\n"
                    
                except Exception as entry_error:
                    logging.warning(f"Entry {idx} failed: {entry_error}")
                    yield f"data: {json.dumps({'type': 'entry', 'data': {'title': entry.get('title', f'Entry {idx+1}'), '_error': str(entry_error), '_index': idx}})}\n\n"
            
            # 5. Done
            yield f"data: {json.dumps({'type': 'done', 'data': {'total': len(entries)}})}\n\n"
            logging.info(f"‚úÖ [ImportAPI] Document stream completed: {len(entries)} entries")
            
        except Exception as e:
            logging.error(f"‚ùå [ImportAPI] Document stream error: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': str(e)}})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

def _build_summary(row: dict) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà extract ‡πÑ‡∏î‡πâ"""
    parts = []
    
    if row.get("highlights"):
        parts.append(f"‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô: {row['highlights']}")
    if row.get("atmosphere"):
        parts.append(f"‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®: {row['atmosphere']}")
    if row.get("location_text"):
        parts.append(f"‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á: {row['location_text']}")
    if row.get("opening_hours"):
        parts.append(f"‡πÄ‡∏õ‡∏¥‡∏î: {row['opening_hours']}")
    if row.get("price_range"):
        parts.append(f"‡∏£‡∏≤‡∏Ñ‡∏≤: {row['price_range']}")
    if row.get("contact"):
        parts.append(f"‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠: {row['contact']}")
    
    return " | ".join(parts) if parts else "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å Bulk Import"


def _extract_keywords(row: dict) -> list:
    """‡∏î‡∏∂‡∏á keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á schema ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà"""
    keywords = []
    
    # Title/Name
    title = row.get("title") or row.get("name")
    if title:
        keywords.append(title)
    
    # Category
    if row.get("category"):
        keywords.append(row["category"])
    
    # Topic/SubTopic    
    topic = row.get("topic") or row.get("sub_topic")
    if topic:
        keywords.append(topic)
    
    # Keywords field (comma separated string or already a list)
    if row.get("keywords"):
        if isinstance(row["keywords"], list):
            keywords.extend(row["keywords"][:5])
        else:
            kw_list = str(row["keywords"]).split(",")
            keywords.extend([k.strip() for k in kw_list[:5]])
    
    # Highlights
    highlights = row.get("detail_highlights") or row.get("highlights")
    if highlights:
        hl_list = str(highlights).split(",")
        keywords.extend([h.strip() for h in hl_list[:3]])
    
    return list(set([k for k in keywords if k]))[:10]  # Max 10 unique keywords


def _build_details(row: dict) -> list:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á details array ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á detail_ prefix ‡πÅ‡∏•‡∏∞ field ‡∏ï‡∏£‡∏á"""
    details = []
    
    # Detail field mappings: (key_prefix, heading)
    detail_mappings = [
        ("detail_overview", "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°"),
        ("detail_location", "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á"),
        ("detail_hours_contact", "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠"),
        ("detail_highlights", "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô"),
        ("detail_price", "‡∏£‡∏≤‡∏Ñ‡∏≤"),
        ("detail_atmosphere", "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®"),
        ("detail_facilities", "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å"),
        ("detail_tips", "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö"),
    ]
    
    # Process detail_ prefixed fields
    for key, heading in detail_mappings:
        if row.get(key):
            details.append({
                "heading": heading,
                "content": str(row[key])
            })
    
    # Fallback to old field names if no detail_ fields found
    if not details:
        info_parts = []
        if row.get("location_text"):
            info_parts.append(f"‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á: {row['location_text']}")
        if row.get("coordinates"):
            info_parts.append(f"‡∏û‡∏¥‡∏Å‡∏±‡∏î: {row['coordinates']}")
        if row.get("opening_hours"):
            info_parts.append(f"‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î: {row['opening_hours']}")
        if row.get("contact"):
            info_parts.append(f"‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠: {row['contact']}")
        if row.get("price_range"):
            info_parts.append(f"‡∏£‡∏≤‡∏Ñ‡∏≤: {row['price_range']}")
        
        if info_parts:
            details.append({"heading": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "content": "\n".join(info_parts)})
        
        if row.get("highlights"):
            details.append({"heading": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô", "content": row["highlights"]})
        if row.get("atmosphere"):
            details.append({"heading": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®", "content": row["atmosphere"]})
        if row.get("facilities"):
            details.append({"heading": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "content": row["facilities"]})
    
    # Handle custom fields (custom_ prefix)
    for key, value in row.items():
        if key.startswith("custom_") and value:
            heading = key.replace("custom_", "").replace("_", " ").title()
            details.append({"heading": heading, "content": str(value)})
    
    return details if details else [{"heading": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°", "content": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å Bulk Import"}]


@router.post("/import-jsonl-stream", tags=["Admin :: Bulk Import"])
async def import_jsonl_stream(
    file: UploadFile = File(..., description="‡πÑ‡∏ü‡∏•‡πå JSONL (.jsonl) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•"),
    db: MongoDBManager = Depends(get_mongo_manager),
    vector_db: QdrantManager = Depends(get_qdrant_manager)
):
    """
    üåä Streaming Import for Massive JSONL Data (Supports 200MB+)
    
    - ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (Memory Efficient)
    - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á DB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (Real-time Insert)
    - Bypass AI ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (Fastest)
    - Syncs to Qdrant (Vector DB) for AI
    """
    import json
    import re
    
    if not file.filename.endswith('.jsonl') and not file.filename.endswith('.json'):
         raise HTTPException(status_code=400, detail="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .jsonl ‡∏´‡∏£‡∏∑‡∏≠ .json ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")

    async def processed_stream():
        count = 0
        success = 0
        failed = 0
        
        # Buffer for parsing lines
        buffer = b""
        
        try:
            # Yield initialization
            yield json.dumps({"type": "start", "message": "Starting stream import..."}) + "\n"
            
            while True:
                chunk = await file.read(1024 * 1024) # Read 1MB chunks
                if not chunk:
                    break
                
                buffer += chunk
                while b'\n' in buffer:
                    line, buffer = buffer.split(b'\n', 1)
                    if not line.strip(): continue
                    
                    count += 1
                    try:
                        # 1. Parse JSON Line
                        record = json.loads(line.decode('utf-8'))
                        
                        # 2. Add minimal defaults if needed
                        if "title" not in record:
                            record["title"] = f"Imported Item {count}"
                        
                        # 3. Save to MongoDB (Directly)
                        # Generate slug
                        slug = record.get("slug", "")
                        if not slug:
                            slug = re.sub(r'[^a-zA-Z0-9\u0E00-\u0E7F\s-]', '', record["title"].lower())
                            slug = re.sub(r'[\s]+', '-', slug.strip())
                        
                        # Ensure uniqueness (simple append)
                        doc = {
                            "slug": slug,
                            **record,
                            "imported_via": "stream_jsonl"
                        }
                        
                        # Save to MongoDB
                        db.add_location(doc)
                        
                        # 4. Sync to Qdrant (Vector DB) -> AI BRAIN üß†
                        try:
                           # Construct text representation for embedding
                           desc_parts = [
                               doc.get('title', ''),
                               doc.get('category', ''),
                               doc.get('topic', ''),
                               doc.get('summary', '')
                           ]
                           if 'details' in doc and isinstance(doc['details'], list):
                               for d in doc['details']:
                                   desc_parts.append(f"{d.get('heading','')}: {d.get('content','')}")
                           
                           text_for_ai = " ".join([str(p) for p in desc_parts if p])
                           
                           # Upsert to Qdrant
                           vector_db.upsert_location(doc_id=str(doc.get('_id', slug)), description=text_for_ai, payload=doc)
                           
                        except Exception as v_err:
                           # Log vector error but don't fail the import
                           print(f"‚ö†Ô∏è Vector DB Sync Error for {slug}: {v_err}")

                        success += 1
                        
                        # Yield progress every 100 items
                        if success % 100 == 0:
                             yield json.dumps({"type": "progress", "count": count, "success": success}) + "\n"

                    except Exception as e:
                        failed += 1
                        yield json.dumps({"type": "error", "row": count, "message": str(e)}) + "\n"
        
            # Final stats
            yield json.dumps({
                "type": "complete", 
                "total": count, 
                "success": success, 
                "failed": failed,
                "message": f"Import Completed! Success: {success}, Failed: {failed}"
            }) + "\n"
            
        except Exception as e:
             yield json.dumps({"type": "fatal_error", "message": str(e)}) + "\n"

    return StreamingResponse(processed_stream(), media_type="application/x-ndjson")
