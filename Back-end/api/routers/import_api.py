# /api/routers/import_api.py
"""
API Router ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI-Powered Smart ETL System
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Import ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel/CSV ‡πÅ‡∏•‡∏∞ AI Transformation
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, File, UploadFile, HTTPException, Body, Depends
from pydantic import BaseModel, Field

from core.services.import_service import import_service
from core.database.mongodb_manager import MongoDBManager
from core.database.qdrant_manager import QdrantManager
from ..dependencies import get_mongo_manager, get_qdrant_manager

router = APIRouter(tags=["Admin :: Bulk Import"])


# =============================================================================
# Pydantic Models
# =============================================================================

class RawPreviewResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö raw file preview"""
    columns: List[str] = Field(..., description="‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ columns ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
    preview_rows: List[Dict[str, Any]] = Field(..., description="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (max 10 rows)")
    total_rows: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    showing_rows: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á")
    filename: str = Field(..., description="‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö")


class AITransformRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI transformation"""
    raw_data: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å preview")
    target_fields: List[str] = Field(
        ..., 
        description="Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏õ",
        min_length=1,
        max_length=15
    )


class AITransformResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI transformation result"""
    original_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö)")
    transformed_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß")
    target_fields: List[str] = Field(..., description="Fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
    total_processed: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")


class ConfirmSaveRequest(BaseModel):
    """Request model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö confirm ‡πÅ‡∏•‡∏∞ save ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    transformed_rows: List[Dict[str, Any]] = Field(..., description="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡∏û‡∏£‡πâ‡∏≠‡∏° save)")


class ConfirmSaveResponse(BaseModel):
    """Response model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö save result"""
    success: bool
    saved_count: int = Field(..., description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà save ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    failed_count: int = Field(0, description="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà save ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    message: str

# =============================================================================
# Target Fields Configuration - ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö MongoDB Schema ‡∏à‡∏£‡∏¥‡∏á
# =============================================================================

# Core Fields - ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô Database
CORE_FIELDS = [
    {
        "key": "title", 
        "label": "Title (‡∏ä‡∏∑‡πà‡∏≠)", 
        "description": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà/‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "category", 
        "label": "Category (‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)", 
        "description": "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å, ‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£, ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß, ‡∏ß‡∏±‡∏î",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "topic", 
        "label": "Topic (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏¢‡πà‡∏≠‡∏¢)", 
        "description": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà, ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠, ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        "type": "text",
        "required": True,
        "group": "core"
    },
    {
        "key": "summary", 
        "label": "Summary (‡∏™‡∏£‡∏∏‡∏õ)", 
        "description": "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß",
        "type": "textarea",
        "required": False,
        "group": "core"
    },
    {
        "key": "keywords", 
        "label": "Keywords (‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤)", 
        "description": "‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ comma ‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏±‡∏î,‡∏ô‡πà‡∏≤‡∏ô,‡∏à‡∏¥‡∏ï‡∏£‡∏Å‡∏£‡∏£‡∏°",
        "type": "tags",
        "required": False,
        "group": "core"
    },
]

# Detail Fields - ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô details[] item)
DETAIL_FIELDS = [
    {
        "key": "detail_overview",
        "label": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
        "description": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤",
        "type": "detail",
        "heading": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
        "group": "details"
    },
    {
        "key": "detail_location",
        "label": "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "description": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà ‡∏û‡∏¥‡∏Å‡∏±‡∏î GPS ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "type": "detail",
        "heading": "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
        "group": "details"
    },
    {
        "key": "detail_hours_contact",
        "label": "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠",
        "description": "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£ Line Facebook",
        "type": "detail",
        "heading": "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠",
        "group": "details"
    },
    {
        "key": "detail_highlights",
        "label": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î",
        "description": "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥/‡∏î‡∏π",
        "type": "detail",
        "heading": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î",
        "group": "details"
    },
    {
        "key": "detail_price",
        "label": "‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
        "description": "‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏° ‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏´‡∏≤‡∏£",
        "type": "detail",
        "heading": "‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
        "group": "details"
    },
    {
        "key": "detail_atmosphere",
        "label": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå",
        "description": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà",
        "type": "detail",
        "heading": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå",
        "group": "details"
    },
    {
        "key": "detail_facilities",
        "label": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "description": "‡∏ó‡∏µ‡πà‡∏à‡∏≠‡∏î‡∏£‡∏ñ WiFi ‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥ ‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "type": "detail",
        "heading": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
        "group": "details"
    },
    {
        "key": "detail_tips",
        "label": "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "description": "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏°‡∏≤‡πÄ‡∏¢‡∏∑‡∏≠‡∏ô ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
        "type": "detail",
        "heading": "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "group": "details"
    },
]

# ‡∏£‡∏ß‡∏° Fields ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
ALL_CONFIGURABLE_FIELDS = CORE_FIELDS + DETAIL_FIELDS


# =============================================================================
# API Endpoints
# =============================================================================

@router.get("/target-fields", tags=["Admin :: Bulk Import"])
async def get_target_fields():
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Target Fields ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°
    - core: ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏´‡∏•‡∏±‡∏Å (title, category, topic, summary, keywords)
    - details: ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô details[] item)
    """
    return {
        "core_fields": CORE_FIELDS,
        "detail_fields": DETAIL_FIELDS,
        "all_fields": ALL_CONFIGURABLE_FIELDS,
        "total": len(ALL_CONFIGURABLE_FIELDS)
    }


@router.post("/preview-raw", response_model=RawPreviewResponse, tags=["Admin :: Bulk Import"])
async def preview_raw_file(file: UploadFile = File(...)):
    """
    üì§ Upload ‡πÅ‡∏•‡∏∞ Preview ‡πÑ‡∏ü‡∏•‡πå Excel/CSV
    
    ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á preview ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏Å‡πà‡∏≠‡∏ô AI transform
    """
    # Validate file type
    if not file.filename:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå")
    
    filename_lower = file.filename.lower()
    valid_extensions = ['.xlsx', '.xls', '.csv']
    
    if not any(filename_lower.endswith(ext) for ext in valid_extensions):
        raise HTTPException(
            status_code=400, 
            detail=f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ: {file.filename}. ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Excel (.xlsx, .xls) ‡πÅ‡∏•‡∏∞ CSV (.csv)"
        )
    
    try:
        # Read file content
        file_content = await file.read()
        
        if not file_content:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
        
        # Parse file in thread pool (blocking operation)
        raw_data = await asyncio.to_thread(
            import_service.parse_file,
            file_content,
            file.filename
        )
        
        if not raw_data:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
        
        # Generate preview
        preview = import_service.get_preview(raw_data, max_rows=10)
        
        return RawPreviewResponse(
            columns=preview["columns"],
            preview_rows=preview["preview_rows"],
            total_rows=preview["total_rows"],
            showing_rows=preview["showing_rows"],
            filename=file.filename
        )
        
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] Error previewing file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {str(e)}")


@router.post("/ai-transform", response_model=AITransformResponse, tags=["Admin :: Bulk Import"])
async def ai_transform_data(request: AITransformRequest):
    """
    ü§ñ AI Transform: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏•‡∏á Target Fields ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    
    ‡πÉ‡∏ä‡πâ Gemini AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞ extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ row
    """
    if not request.raw_data:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    
    if not request.target_fields:
        raise HTTPException(status_code=400, detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Target Fields ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 field")
    
    # Validate target fields - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á core ‡πÅ‡∏•‡∏∞ detail fields
    valid_field_keys = [f["key"] for f in ALL_CONFIGURABLE_FIELDS]
    for field in request.target_fields:
        # Custom fields (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ custom_) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á validate
        if field.startswith("custom_"):
            continue
        if field not in valid_field_keys:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid field: {field}. Valid fields: {valid_field_keys}"
            )
    
    try:
        # Import AI Mapper Service
        from core.services.ai_mapper_service import ai_mapper_service
        
        # Process with AI
        logging.info(f"ü§ñ [ImportAPI] Starting AI transform for {len(request.raw_data)} rows, fields: {request.target_fields}")
        
        transformed = await ai_mapper_service.transform_batch(
            rows=request.raw_data,
            target_fields=request.target_fields,
            concurrency=3  # Process 3 rows at a time
        )
        
        logging.info(f"‚úÖ [ImportAPI] AI transform completed: {len(transformed)} rows processed")
        
        return AITransformResponse(
            original_rows=request.raw_data,
            transformed_rows=transformed,
            target_fields=request.target_fields,
            total_processed=len(transformed)
        )
        
    except Exception as e:
        logging.error(f"‚ùå [ImportAPI] AI transform error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail=f"AI Transform ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}"
        )


@router.post("/confirm-save", response_model=ConfirmSaveResponse, tags=["Admin :: Bulk Import"])
async def confirm_save_data(
    request: ConfirmSaveRequest,
    db: MongoDBManager = Depends(get_mongo_manager),
    vector_db: QdrantManager = Depends(get_qdrant_manager)
):
    """
    üíæ Confirm ‡πÅ‡∏•‡∏∞ Save ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà AI ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á Database
    
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Vector ‡πÉ‡∏ô Qdrant
    """
    if not request.transformed_rows:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
    
    saved_count = 0
    failed_count = 0
    errors = []
    
    for idx, row in enumerate(request.transformed_rows):
        try:
            # Remove internal fields
            clean_row = {k: v for k, v in row.items() if not k.startswith("_")}
            
            # Generate slug from title (new schema) or name (old)
            title = clean_row.get("title") or clean_row.get("name", "")
            if not title:
                title = f"imported-item-{idx + 1}"
            
            # Create slug (kebab-case)
            import re
            slug = re.sub(r'[^a-zA-Z0-9\u0E00-\u0E7F\s-]', '', title.lower())
            slug = re.sub(r'[\s]+', '-', slug.strip())
            slug = slug[:50]  # Limit length
            if not slug:
                slug = f"item-{idx + 1}"
            
            # Check for duplicate slug, add suffix if needed
            existing = await asyncio.to_thread(db.get_location_by_slug, slug)
            if existing:
                import time
                slug = f"{slug}-{int(time.time()) % 10000}"
            
            # Build location document matching new schema
            location_doc = {
                "slug": slug,
                "title": title,
                "category": clean_row.get("category", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"),
                "topic": clean_row.get("topic") or clean_row.get("sub_topic", "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"),
                "summary": clean_row.get("summary") or _build_summary(clean_row),
                "keywords": _extract_keywords(clean_row),
                "details": _build_details(clean_row),
                "metadata": {
                    "image_prefix": slug,
                    "imported_via": "bulk_import",
                    "source_fields": list(clean_row.keys())
                }
            }
            
            # Save to MongoDB
            mongo_id = await asyncio.to_thread(
                db.add_location,
                location_doc
            )
            
            if not mongo_id:
                raise Exception("MongoDB insert returned None")
            
            # Create vector in Qdrant
            try:
                desc_for_vector = f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {location_doc['title']}\n‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {location_doc['topic']}\n‡∏™‡∏£‡∏∏‡∏õ: {location_doc['summary']}"
                await vector_db.upsert_location(mongo_id=mongo_id, description=desc_for_vector)
            except Exception as ve:
                logging.warning(f"‚ö†Ô∏è Vector creation failed for {mongo_id}: {ve}")
            
            saved_count += 1
            logging.info(f"‚úÖ Saved: {location_doc['title']} (slug: {slug})")
            
        except Exception as e:
            failed_count += 1
            error_msg = f"Row {idx + 1}: {str(e)}"
            errors.append(error_msg)
            logging.error(f"‚ùå Failed to save row {idx + 1}: {e}")
    
    # Build result message
    if failed_count == 0:
        message = f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {saved_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£!"
    else:
        message = f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {saved_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß {failed_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
        if errors:
            message += f". Errors: {'; '.join(errors[:3])}"
    
    return ConfirmSaveResponse(
        success=failed_count == 0,
        saved_count=saved_count,
        failed_count=failed_count,
        message=message
    )


def _build_summary(row: dict) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà extract ‡πÑ‡∏î‡πâ"""
    parts = []
    
    if row.get("highlights"):
        parts.append(f"‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô: {row['highlights']}")
    if row.get("atmosphere"):
        parts.append(f"‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®: {row['atmosphere']}")
    if row.get("location_text"):
        parts.append(f"‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á: {row['location_text']}")
    if row.get("opening_hours"):
        parts.append(f"‡πÄ‡∏õ‡∏¥‡∏î: {row['opening_hours']}")
    if row.get("price_range"):
        parts.append(f"‡∏£‡∏≤‡∏Ñ‡∏≤: {row['price_range']}")
    if row.get("contact"):
        parts.append(f"‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠: {row['contact']}")
    
    return " | ".join(parts) if parts else "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å Bulk Import"


def _extract_keywords(row: dict) -> list:
    """‡∏î‡∏∂‡∏á keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á schema ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà"""
    keywords = []
    
    # Title/Name
    title = row.get("title") or row.get("name")
    if title:
        keywords.append(title)
    
    # Category
    if row.get("category"):
        keywords.append(row["category"])
    
    # Topic/SubTopic    
    topic = row.get("topic") or row.get("sub_topic")
    if topic:
        keywords.append(topic)
    
    # Keywords field (comma separated string or already a list)
    if row.get("keywords"):
        if isinstance(row["keywords"], list):
            keywords.extend(row["keywords"][:5])
        else:
            kw_list = str(row["keywords"]).split(",")
            keywords.extend([k.strip() for k in kw_list[:5]])
    
    # Highlights
    highlights = row.get("detail_highlights") or row.get("highlights")
    if highlights:
        hl_list = str(highlights).split(",")
        keywords.extend([h.strip() for h in hl_list[:3]])
    
    return list(set([k for k in keywords if k]))[:10]  # Max 10 unique keywords


def _build_details(row: dict) -> list:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á details array ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á detail_ prefix ‡πÅ‡∏•‡∏∞ field ‡∏ï‡∏£‡∏á"""
    details = []
    
    # Detail field mappings: (key_prefix, heading)
    detail_mappings = [
        ("detail_overview", "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°"),
        ("detail_location", "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á"),
        ("detail_hours_contact", "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠"),
        ("detail_highlights", "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô"),
        ("detail_price", "‡∏£‡∏≤‡∏Ñ‡∏≤"),
        ("detail_atmosphere", "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®"),
        ("detail_facilities", "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å"),
        ("detail_tips", "‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö"),
    ]
    
    # Process detail_ prefixed fields
    for key, heading in detail_mappings:
        if row.get(key):
            details.append({
                "heading": heading,
                "content": str(row[key])
            })
    
    # Fallback to old field names if no detail_ fields found
    if not details:
        info_parts = []
        if row.get("location_text"):
            info_parts.append(f"‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á: {row['location_text']}")
        if row.get("coordinates"):
            info_parts.append(f"‡∏û‡∏¥‡∏Å‡∏±‡∏î: {row['coordinates']}")
        if row.get("opening_hours"):
            info_parts.append(f"‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î: {row['opening_hours']}")
        if row.get("contact"):
            info_parts.append(f"‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠: {row['contact']}")
        if row.get("price_range"):
            info_parts.append(f"‡∏£‡∏≤‡∏Ñ‡∏≤: {row['price_range']}")
        
        if info_parts:
            details.append({"heading": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "content": "\n".join(info_parts)})
        
        if row.get("highlights"):
            details.append({"heading": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô", "content": row["highlights"]})
        if row.get("atmosphere"):
            details.append({"heading": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®", "content": row["atmosphere"]})
        if row.get("facilities"):
            details.append({"heading": "‡∏™‡∏¥‡πà‡∏á‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "content": row["facilities"]})
    
    # Handle custom fields (custom_ prefix)
    for key, value in row.items():
        if key.startswith("custom_") and value:
            heading = key.replace("custom_", "").replace("_", " ").title()
            details.append({"heading": heading, "content": str(value)})
    
    return details if details else [{"heading": "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°", "content": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å Bulk Import"}]
