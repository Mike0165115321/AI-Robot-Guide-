# /core/ai_models/llm_handler.py (V5 - Hybrid Fallback System)

import google.generativeai as genai
from groq import AsyncGroq 
import logging 
import asyncio 
import json 
from typing import Dict, Any, Optional

from core.config import settings
from .key_manager import gemini_key_manager, groq_key_manager 

async def get_llama_response_direct_async(user_query: str) -> str:
    # (‡∏™‡πà‡∏ß‡∏ô Small Talk ‡πÉ‡∏ä‡πâ Groq ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏≤‡∏á‡πÜ)
    api_key = groq_key_manager.get_key()
    if not api_key: 
        return "‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞"
        
    try:
        client = AsyncGroq(api_key=api_key)
        system_prompt_small_talk = "You are 'Nong Nan', a cheerful AI tour guide. Keep responses short and friendly in Thai."
        chat_completion = await client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt_small_talk},
                {"role": "user", "content": user_query}
            ],
            model="llama-3.1-8b-instant",
            temperature=0.7, 
            max_tokens=100
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        logging.error(f"‚ùå [Small Talk] Groq Error: {e}")
        return "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏∂‡∏ô‡∏´‡∏±‡∏ß‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏•‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?"

def get_insights_from_logs(log_collection) -> dict:
    if log_collection is None:
        return {}
    try:
        top_topics_cursor = log_collection.aggregate([
            {"$group": {"_id": "$primary_topic", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 3}
        ])
        top_topics = [item["_id"] for item in top_topics_cursor if item.get("_id")]
        return {"top_topics": top_topics} if top_topics else {}
    except Exception:
        return {}

def _generate_rag_prompts(user_query: str, context: str, insights: dict) -> Dict[str, str]:
    insights_text = "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏Ñ‡πà‡∏∞"
    if top_topics := insights.get("top_topics"):
        top_topics_str = ", ".join(top_topics)
        insights_text = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï‡∏Ñ‡∏∑‡∏≠ {top_topics_str}"
    
    system_prompt = f"""# ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô" ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô (AI) ‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏£‡πà‡∏≤‡πÄ‡∏£‡∏¥‡∏á ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á
‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÇ‡∏î‡∏¢ **‡∏ï‡πâ‡∏≠‡∏á** ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" (Context) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

# ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å
1.  **‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:** {{ "answer": "...", "sources_used": [...] }}
2.  **‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:** ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Context ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö
3.  **Sources:** ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà (title) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡∏•‡∏á‡πÉ‡∏ô List "sources_used"

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏™‡∏£‡∏¥‡∏°
{insights_text}

# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON Output
{{
"answer": "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (Markdown)",
"sources_used": ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà 1", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà 2"]
}}
"""
    user_prompt = f"""# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö (Context)
---
{context}
---

# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_query}
# ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö JSON:
"""
    return {"system": system_prompt.strip(), "user": user_prompt.strip()}

async def _get_gemini_fallback(prompts: Dict[str, str]) -> dict:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Groq ‡∏•‡πà‡∏°"""
    api_key = gemini_key_manager.get_key()
    if not api_key:
        logging.error("‚ùå [Fallback] No Gemini API Key available.")
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏∏‡∏ç‡πÅ‡∏à‡∏™‡∏≥‡∏£‡∏≠‡∏á", "sources_used": []}

    try:
        logging.info(f"üõ°Ô∏è [Fallback] Switching to Gemini ({settings.GEMINI_MODEL})...")
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        # Gemini ‡∏ä‡∏≠‡∏ö Prompt ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô
        full_prompt = f"{prompts['system']}\n\n{prompts['user']}"
        
        response = await asyncio.to_thread(
            model.generate_content,
            full_prompt,
            generation_config={"response_mime_type": "application/json"}
        )
        
        return json.loads(response.text)
    except Exception as e:
        logging.error(f"‚ùå [Fallback] Gemini Error: {e}")
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß", "sources_used": []}

async def get_groq_rag_response_async(user_query: str, context: str, insights: dict) -> dict:
    prompts = _generate_rag_prompts(user_query, context, insights)
    api_key = groq_key_manager.get_key()

    # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ Groq ‡∏Å‡πà‡∏≠‡∏ô (Model ‡∏´‡∏•‡∏±‡∏Å)
    if api_key:
        try:
            client = AsyncGroq(api_key=api_key)
            logging.info(f"ü§ñ [LLM-RAG] Calling Groq ({settings.GROQ_LLAMA_MODEL})...")
            
            chat_completion = await client.chat.completions.create(
                messages=[
                    {"role": "system", "content": prompts["system"]},
                    {"role": "user", "content": prompts["user"]}
                ],
                model=settings.GROQ_LLAMA_MODEL, 
                temperature=0.3, 
                max_tokens=4096,
                response_format={"type": "json_object"},
            )
            return json.loads(chat_completion.choices[0].message.content)
            
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è [LLM-RAG] Groq Failed: {e}")
            # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏•‡∏á‡πÑ‡∏õ‡∏ó‡∏≥ Fallback ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á

    # 2. ‡∏ñ‡πâ‡∏≤ Groq ‡∏û‡∏±‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ Key -> ‡πÉ‡∏ä‡πâ Gemini ‡πÅ‡∏ó‡∏ô
    return await _get_gemini_fallback(prompts)