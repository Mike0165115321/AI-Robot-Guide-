# Back-end/core/ai_models/llm_handler.py

import logging
import os
import json
from typing import List, Dict, Any, Optional
from groq import AsyncGroq
from core.config import settings

groq_client = AsyncGroq(api_key=settings.GROQ_API_KEYS[0])

async def get_llm_response(
    messages: List[Dict[str, str]], 
    model_name: str = settings.GROQ_LLAMA_MODEL,
    temperature: float = 0.3,
    max_tokens: int = 1024,
    json_mode: bool = False
) -> str:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    ‡∏£‡∏±‡∏ö messages ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á dict ‡πÄ‡∏ä‡πà‡∏ô:
    [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
    """
    try:
        kwargs = {
            "model": model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if json_mode:
            kwargs["response_format"] = {"type": "json_object"}

        response = await groq_client.chat.completions.create(**kwargs)
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"‚ùå [LLM Handler] Error calling Groq: {e}")
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (LLM Error)"

async def get_llama_response_direct_async(user_query: str) -> str:
    """
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Small Talk / ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏°‡∏µ system prompt ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö
    """
    system_prompt = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô ‡∏û‡∏π‡∏î‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á

‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
1. ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö (2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
2. ‡∏≠‡∏¢‡πà‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö - ‡πÅ‡∏Ñ‡πà‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏ö
3. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ô‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô
4. ‡∏≠‡∏¢‡πà‡∏≤‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏™‡∏ô‡πÉ‡∏à‡∏≠‡∏∞‡πÑ‡∏£" ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ:
- "‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏à‡∏µ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö" ‚Üí "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏∞! ‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÑ‡∏ó‡∏•‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ô‡∏∞‡∏Ñ‡∏∞ üéâ"
- "‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û" ‚Üí "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏∞! ‡∏´‡∏ô‡∏µ‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏≤‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏ô‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞ üòä"
"""
    
    return await get_llm_response(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ],
        model_name=settings.GROQ_SMALL_TALK_MODEL,
        temperature=0.7 
    )

async def get_groq_rag_response_async(user_query: str, context: str, insights: str = "", turn_count: int = 1) -> Dict[str, Any]:
    system_msg = f"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô ‡πÑ‡∏Å‡∏î‡πå‡∏ô‡∏≥‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{context}"
    if insights:
        system_msg += f"\n\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:\n{insights}"

    response_text = await get_llm_response([
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_query}
    ])
    
    return {"answer": response_text, "sources_used": []}