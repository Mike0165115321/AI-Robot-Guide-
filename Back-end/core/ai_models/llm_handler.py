# /core/ai_models/llm_handler.py

import google.generativeai as genai
from groq import AsyncGroq 
import logging 
import asyncio 
import json 
from typing import Dict, Any, Optional

from core.config import settings
from .key_manager import gemini_key_manager, groq_key_manager 

async def get_llama_response_direct_async(user_query: str) -> str:
    api_key = groq_key_manager.get_key()
    if not api_key: 
        return "‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"
        
    try:
        client = AsyncGroq(api_key=api_key)
        system_prompt_small_talk = "You are 'Nong Nan', a helpful and knowledgeable AI tour guide for Nan province, Thailand. Reply in polite, natural Thai."
        chat_completion = await client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt_small_talk},
                {"role": "user", "content": user_query}
            ],
            model=settings.GROQ_SMALL_TALK_MODEL, 
            temperature=0.7, 
            max_tokens=150
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        logging.error(f"‚ùå [Small Talk] Groq Error: {e}")
        return "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏°‡∏∂‡∏ô‡∏á‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ï‡πà‡∏≠‡πÄ‡∏•‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"

def get_insights_from_logs(log_collection) -> dict:
    if log_collection is None:
        return {}
    try:
        top_topics_cursor = log_collection.aggregate([
            {"$group": {"_id": "$primary_topic", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 3}
        ])
        top_topics = [item["_id"] for item in top_topics_cursor if item.get("_id")]
        return {"top_topics": top_topics} if top_topics else {}
    except Exception:
        return {}

def _generate_rag_prompts(user_query: str, context: str, insights: dict, turn_count: int) -> Dict[str, str]:
    insights_text = ""
    if top_topics := insights.get("top_topics"):
        top_topics_str = ", ".join(top_topics)
        insights_text = f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï: {top_topics_str}"
    
    outro_strategy = ""
    
    if turn_count > 0 and turn_count % 3 == 0:
        outro_strategy = """
    - **[‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ]:** ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏™‡πà‡πÉ‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©
    - **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏≥‡∏ô‡∏≠‡∏á‡∏ß‡πà‡∏≤:** "‡πÄ‡∏™‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏•‡πâ‡∏ß ‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏∞ üòÜ ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô **‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏û‡∏•‡∏á** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÜ ‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞?" (‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô)"""
    else:
        outro_strategy = """
    - ‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏ä‡πà‡∏ô "‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡∏ä‡πà‡∏ß‡∏¢ **‡∏ô‡∏≥‡∏ó‡∏≤‡∏á** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏•‡∏¢‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞?" ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß"""

    system_prompt = f"""# ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ **"‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô"** ‡πÑ‡∏Å‡∏î‡πå‡∏™‡∏≤‡∏ß‡πÄ‡∏à‡πâ‡∏≤‡∏ñ‡∏¥‡πà‡∏ô (AI) ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ ‡πÉ‡∏™‡πà‡πÉ‡∏à ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏¢‡∏™‡∏ô‡∏∏‡∏Å (‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥)

# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (The Flow)
1.  **Intro:** ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô
2.  **Content:** ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å [Context] ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
    - **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡πÅ‡∏ó‡∏£‡∏Å Tag ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û `{{{{IMAGE: ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà}}}}` ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
3.  **Outro (‡∏™‡∏£‡∏∏‡∏õ):** - ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏ä‡∏µ‡∏¢‡∏£‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
    {outro_strategy}

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
"‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞! ... (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà) ...
{{{{IMAGE: ‡∏î‡∏≠‡∏¢‡πÄ‡∏™‡∏°‡∏≠‡∏î‡∏≤‡∏ß}}}}
... (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πà‡∏≠) ...

‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏ô‡∏∞‡∏Ñ‡∏∞ {("‡πÄ‡∏™‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏•‡πâ‡∏ß ‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏∞ üòÜ ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô **‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏û‡∏•‡∏á** ‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ä‡∏¥‡∏•‡πÜ ‡∏î‡∏µ‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞?" if turn_count > 0 and turn_count % 3 == 0 else "‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô **‡∏ô‡∏≥‡∏ó‡∏≤‡∏á** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞")}"

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏™‡∏£‡∏¥‡∏°
{insights_text}

# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON Output
{{
"answer": "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (Markdown ‡∏û‡∏£‡πâ‡∏≠‡∏° Tag ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û)",
"sources_used": ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà 1", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà 2"]
}}
"""
    
    user_prompt = f"""# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö (Context)
---
{context}
---

# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "{user_query}"

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:** ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡πÅ‡∏ó‡∏£‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞ **‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà {turn_count}**

# ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö JSON:
"""
    return {"system": system_prompt.strip(), "user": user_prompt.strip()}

async def _get_gemini_fallback(prompts: Dict[str, str]) -> dict:
    api_key = gemini_key_manager.get_key()
    if not api_key:
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß", "sources_used": []}

    try:
        logging.info(f"üõ°Ô∏è [Fallback] Gemini ({settings.GEMINI_MODEL})...")
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(settings.GEMINI_MODEL)
        full_prompt = f"{prompts['system']}\n\n{prompts['user']}"
        response = await asyncio.to_thread(
            model.generate_content,
            full_prompt,
            generation_config={"response_mime_type": "application/json"}
        )
        return json.loads(response.text)
    except Exception as e:
        logging.error(f"‚ùå [Fallback] Gemini Error: {e}")
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", "sources_used": []}

async def get_groq_rag_response_async(user_query: str, context: str, insights: dict, turn_count: int = 1) -> dict:
    
    # ‡∏™‡πà‡∏á turn_count ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt
    prompts = _generate_rag_prompts(user_query, context, insights, turn_count)
    
    api_key = groq_key_manager.get_key()

    if api_key:
        try:
            client = AsyncGroq(api_key=api_key)
            logging.info(f"ü§ñ [LLM-RAG] Calling Groq ({settings.GROQ_LLAMA_MODEL})... Turn: {turn_count}")
            
            chat_completion = await client.chat.completions.create(
                messages=[
                    {"role": "system", "content": prompts["system"]},
                    {"role": "user", "content": prompts["user"]}
                ],
                model=settings.GROQ_LLAMA_MODEL, 
                temperature=0.7,  
                max_tokens=4096,
                response_format={"type": "json_object"},
            )
            return json.loads(chat_completion.choices[0].message.content)
            
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è [LLM-RAG] Groq Failed: {e}")

    return await _get_gemini_fallback(prompts)