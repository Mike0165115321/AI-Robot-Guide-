# /core/ai_models/llm_handler.py (V4 - Smart Sources)

import google.generativeai as genai
from groq import AsyncGroq 
import logging 
import asyncio 
import json 
from typing import Dict, Any, Optional

from core.config import settings
from .key_manager import gemini_key_manager, groq_key_manager 

async def get_llama_response_direct_async(user_query: str) -> str:
    api_key = groq_key_manager.get_key()
    if not api_key: 
        logging.error("[Small Talk] No Groq API key available.")
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Ñ‡πà‡∏∞"
        
    try:
        client = AsyncGroq(api_key=api_key)
        system_prompt_small_talk = """You are 'Nong Nan', a cheerful AI tour guide for Nan province, Thailand. Your task is to engage in simple, positive small talk.
- Keep responses very short (1-2 sentences).
- Always be friendly and polite.
- If appropriate, end with a gentle question to keep the conversation going.

Example:
User: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ
Your response: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞! ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?
"""
        chat_completion = await client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt_small_talk},
                {"role": "user", "content": user_query}
            ],
            model="llama-3.1-8b-instant", # (‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Small Talk)
            temperature=0.7, 
            max_tokens=100
        )
        response_text = chat_completion.choices[0].message.content
        logging.info("‚úÖ [Small Talk] Received direct response from Groq (Async).")
        return response_text
        
    except Exception as e:
        logging.error(f"‚ùå [Small Talk] Error calling Groq API (Async): {e}", exc_info=True)
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡πà‡∏∞"

def get_insights_from_logs(log_collection) -> dict:
    if log_collection is None:
        return {}
    try:
        top_topics_cursor = log_collection.aggregate([
            {"$group": {"_id": "$primary_topic", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 3}
        ])
        top_topics = [item["_id"] for item in top_topics_cursor if item.get("_id")]
        if top_topics:
            logging.info(f"üìà [Analytics] Top topics found: {top_topics}")
            return {"top_topics": top_topics}
        return {}
    except Exception as e:
        logging.error(f"‚ùå [Analytics] Failed to get insights: {e}", exc_info=True)
        return {}


def _generate_llama_rag_prompts(user_query: str, context: str, insights: dict) -> Dict[str, str]:
    insights_text = "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏Ñ‡πà‡∏∞"
    if top_topics := insights.get("top_topics"):
        top_topics_str = ", ".join(top_topics)
        insights_text = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ñ‡∏≤‡∏°‡∏ñ‡∏∂‡∏á‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ {top_topics_str}"
    
    system_prompt = f"""# ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô" ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏£‡πà‡∏≤‡πÄ‡∏£‡∏¥‡∏á ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°
‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß" ‡πÇ‡∏î‡∏¢ **‡∏ï‡πâ‡∏≠‡∏á** ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" (Context) ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

# ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
1.  **‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:** ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì **‡∏ï‡πâ‡∏≠‡∏á** ‡πÄ‡∏õ‡πá‡∏ô JSON object ‡∏ó‡∏µ‡πà‡∏°‡∏µ 2 key ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô: `answer` ‡πÅ‡∏•‡∏∞ `sources_used`.
2.  **Key "answer" (String):**
    * ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° (‡πÉ‡∏ä‡πâ Markdown) ‡πÅ‡∏•‡∏∞‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
    * **‡∏´‡πâ‡∏≤‡∏°** ‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
    * ‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å: ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° "‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô" ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢ "‡∏Ñ‡πà‡∏∞"
    * ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö: ‡∏´‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" **‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î** ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤: "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞"
3.  **Key "sources_used" (List of Strings):**
    * ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ List ‡∏Ç‡∏≠‡∏á "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà" (`title`) ‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    * **‡∏ï‡πâ‡∏≠‡∏á** ‡πÉ‡∏™‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞ `title` ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì "‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ñ‡∏∂‡∏á" ‡πÉ‡∏ô `answer` ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    * ‡∏´‡∏≤‡∏Å `answer` ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞..." (‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ‡πÉ‡∏´‡πâ key ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô List ‡∏ß‡πà‡∏≤‡∏á: `[]`
    * ‡∏´‡∏≤‡∏Å LLM (‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì) ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à "‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ" ‡∏ö‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏™‡∏≤‡∏î‡∏¥‡∏ô‡∏ô‡∏≤‡∏ô‡πâ‡∏≠‡∏¢) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß" ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°... **‡∏´‡πâ‡∏≤‡∏°** ‡πÉ‡∏™‡πà `title` ‡∏ô‡∏±‡πâ‡∏ô‡∏•‡∏á‡πÉ‡∏ô List ‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤)
{insights_text} (‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô "‡πÇ‡∏≠‡πâ! ‡∏ß‡∏±‡∏î‡∏†‡∏π‡∏°‡∏¥‡∏ô‡∏ó‡∏£‡πå‡πÄ‡∏´‡∏£‡∏≠‡∏Ñ‡∏∞ ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ñ‡∏≤‡∏°‡∏ñ‡∏∂‡∏á‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞!")

# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö (‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å JSON ‡∏ô‡∏µ‡πâ)
{{
"answer": "...",
"sources_used": ["(title ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 1 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)", "(title ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 2 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)"]
}}

‡∏à‡∏á‡∏ï‡∏≠‡∏ö "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß" ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö" ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏ô user message
"""

    user_prompt = f"""# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö (Context)
---
{context}
---

# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß
{user_query}

# ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞ '‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô' ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):
"""
    
    return {"system": system_prompt.strip(), "user": user_prompt.strip()}


async def get_groq_rag_response_async(user_query: str, context: str, insights: dict) -> dict:
    """
    (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç) ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary {"answer": ..., "sources_used": ...}
    """
    api_key = groq_key_manager.get_key()
    if not api_key:
        logging.error("[LLM-RAG] No Groq API key available.")
        return {"answer": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq API Key", "sources_used": []} # üëà ‡∏Ñ‡∏∑‡∏ô dict
        
    try:
        prompts = _generate_llama_rag_prompts(user_query, context, insights)
        
        client = AsyncGroq(api_key=api_key)
        
        logging.info(f"ü§ñ [LLM-RAG] Calling Groq RAG API (Async) using Llama 70B (Smart Source Mode)...")
        
        chat_completion = await client.chat.completions.create(
            messages=[
                {"role": "system", "content": prompts["system"]},
                {"role": "user", "content": prompts["user"]}
            ],
            model=settings.GROQ_LLAMA_MODEL, 
            temperature=0.3, 
            max_tokens=4096,
            response_format={"type": "json_object"}, # üëà ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON
        )
        
        response_text = chat_completion.choices[0].message.content
        
        try:
            llm_response_dict = json.loads(response_text)
            if "answer" not in llm_response_dict or "sources_used" not in llm_response_dict:
                raise ValueError("Missing required keys")
            
            logging.info(f"‚úÖ [LLM-RAG] Received valid JSON response. Sources used: {llm_response_dict.get('sources_used')}")
            return llm_response_dict

        except Exception as e:
            logging.error(f"‚ùå [LLM-RAG] Failed to parse JSON from LLM: {e}. Raw text: '{response_text[:100]}...'")
            return {"answer": response_text, "sources_used": None} 
    except Exception as e:
        logging.error(f"‚ùå [LLM-RAG] Error calling Groq 70B API (Async): {e}", exc_info=True)
        return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏´‡∏•‡∏±‡∏Å", "sources_used": []}