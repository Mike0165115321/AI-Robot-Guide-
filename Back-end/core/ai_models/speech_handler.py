# /core/ai_models/speech_handler.py (Final WebM Handling Version)

import whisper
import io
from gtts import gTTS
import tempfile
from pydub import AudioSegment # <-- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ pydub

class SpeechHandler:
    def __init__(self, model_size="base"):
        print(f"üîÑ [Speech] Loading Whisper model '{model_size}'...")
        self.stt_model = whisper.load_model(model_size)
        print(f"‚úÖ [Speech] Whisper model '{model_size}' loaded successfully.")

    def transcribe_audio_bytes(self, audio_bytes: bytes) -> str:
        """
        [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏ö‡∏ö bytes (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå .webm) ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        """
        if not self.stt_model:
            raise RuntimeError("Whisper model is not loaded.")

        try:
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á file-like object ‡πÉ‡∏ô memory ‡∏à‡∏≤‡∏Å bytes ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≤
            webm_file_like_object = io.BytesIO(audio_bytes)
            
            # 2. ‡πÉ‡∏´‡πâ pydub ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å object ‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏ö‡∏≠‡∏Å‡∏°‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô format 'webm'
            audio_segment = AudioSegment.from_file(webm_file_like_object, format="webm")
            
            # 3. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô Mono, 16kHz WAV ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Whisper
            wav_segment = audio_segment.set_channels(1).set_frame_rate(16000)

            # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå WAV ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Whisper ‡∏≠‡πà‡∏≤‡∏ô
            with tempfile.NamedTemporaryFile(delete=True, suffix=".wav") as temp_wav_file:
                wav_segment.export(temp_wav_file.name, format="wav")
                
                print(f"üé§ [STT] Transcribing converted .wav audio...")
                result = self.stt_model.transcribe(
                    temp_wav_file.name,
                    language="th",
                    fp16=False
                )
                
                transcribed_text = result['text'].strip()
                print(f"üìù [STT] Transcription result: '{transcribed_text}'")
                return transcribed_text

        except Exception as e:
            print(f"‚ùå [STT] Error transcribing .webm data: {e}", exc_info=True)
            return ""

    def synthesize_speech_to_bytes(self, text: str) -> bytes:
        """
        [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô MP3 bytes ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå
        """
        if not text.strip():
            raise ValueError("Input text for TTS cannot be empty.")
        
        print(f"üó£Ô∏è  [TTS] Synthesizing speech to MP3 bytes for text: '{text[:30]}...'")
        
        try:
            # ‡πÉ‡∏ä‡πâ gTTS ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á MP3 ‡πÉ‡∏ô memory
            tts = gTTS(text=text, lang='th', slow=False)
            mp3_fp = io.BytesIO()
            tts.write_to_fp(mp3_fp)
            mp3_fp.seek(0)
            
            # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô MP3 bytes ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            return mp3_fp.read()

        except Exception as e:
            print(f"‚ùå [TTS] Error during MP3 speech synthesis: {e}")
            raise RuntimeError("Failed to synthesize speech to MP3 bytes.")

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Å‡∏•‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ---
speech_handler_instance = SpeechHandler(model_size="base")